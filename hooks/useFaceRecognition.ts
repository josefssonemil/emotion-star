import { useState, useEffect } from 'react';
import * as faceapi from 'face-api.js';

export default function useFaceRecognition(videoRef: any) {
    const [player1, setPlayer1] = useState('');
    const [player2, setPlayer2] = useState('');
    //const [emoji, setEmoji] = useState('ðŸ˜');

    useEffect(() => {
        Promise.all([
            faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
            faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
            faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
            faceapi.nets.faceExpressionNet.loadFromUri('/models')
        ]).then(() => {
            navigator.getUserMedia(
                { video: {} },
                stream => {
                    if (videoRef.current) {
                        videoRef.current.srcObject = stream
                    }
                },
                err => console.error(err)
            )
        });

        function getHighestExpressionValue(val: faceapi.FaceExpressions) {
            const sortedKeys = Object.keys(val).sort((a, b) => {
                const valueA = val[a];
                const valueB = val[b];

                return valueB - valueA;
            });

            return sortedKeys[0];
        }

        function returnEmoji(expression: any){
            if (expression === "happy"){
                return 'ðŸ˜';
            } 
            else if (expression === 'angry'){
                return 'ðŸ˜¡';
            }
            else if (expression === 'surprised'){
                return 'ðŸ˜¯';
            }
            else if (expression === 'sad'){
                return 'ðŸ˜¢';
            }
            else if (expression === 'disgusted'){
                return 'ðŸ¤¢';
            }
            else if (expression === 'fearful'){
                return 'ðŸ˜°';
            }
            else{
                return 'ðŸ˜';
            }
        }

        function onPlay() {
            setInterval(async () => {
                const detections = await faceapi.detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions()).withFaceExpressions()
                if (detections && detections.length > 0) {
                    let player1 = 0;
                    let player2 = 1;

                    if (detections.length === 2 && detections[0].detection.box.x > detections[1].detection.box.x) {
                            player1 = 1;
                            player2 = 0;
                    }


                    const expressions = detections.map(detection => getHighestExpressionValue(detection.expressions));
                    setPlayer1(returnEmoji(expressions[player1]));

                    if (detections.length === 2) {
                        setPlayer2(returnEmoji(expressions[player2]));
                    } else {
                        setPlayer2('');
                    }
                } else {
                    setPlayer1('');
                    setPlayer2('');
                }
            }, 100)
        }

        videoRef.current?.addEventListener('play', onPlay);

        return () => {
            videoRef.current?.removeEventListener('play', onPlay);
        };
    }, [videoRef]);

    return { player1, player2 };
}
